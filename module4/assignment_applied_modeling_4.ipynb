{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCc3XZEyG3XV"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 4*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Model Interpretation 2\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your work.\n",
    "\n",
    "- [ ] Continue to iterate on your project: data cleaning, exploratory visualization, feature engineering, modeling.\n",
    "- [ ] Make a Shapley force plot to explain at least 1 individual prediction.\n",
    "- [ ] Share at least 1 visualization (of any type) on Slack.\n",
    "\n",
    "But, if you aren't ready to make a Shapley force plot with your own dataset today, that's okay. You can practice this objective with another dataset instead. You may choose any dataset you've worked with previously.\n",
    "\n",
    "## Stretch Goals\n",
    "- [ ] Make Shapley force plots to explain at least 4 individual predictions.\n",
    "    - If your project is Binary Classification, you can do a True Positive, True Negative, False Positive, False Negative.\n",
    "    - If your project is Regression, you can do a high prediction with low error, a low prediction with low error, a high prediction with high error, and a low prediction with high error.\n",
    "- [ ] Use Shapley values to display verbal explanations of individual predictions.\n",
    "- [ ] Use the SHAP library for other visualization types.\n",
    "\n",
    "The [SHAP repo](https://github.com/slundberg/shap) has examples for many visualization types, including:\n",
    "\n",
    "- Force Plot, individual predictions\n",
    "- Force Plot, multiple predictions\n",
    "- Dependence Plot\n",
    "- Summary Plot\n",
    "- Summary Plot, Bar\n",
    "- Interaction Values\n",
    "- Decision Plots\n",
    "\n",
    "We just did the first type during the lesson. The [Kaggle microcourse](https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values) shows two more. Experiment and see what you can learn!\n",
    "\n",
    "\n",
    "## Links\n",
    "- [Kaggle / Dan Becker: Machine Learning Explainability — SHAP Values](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "- [Christoph Molnar: Interpretable Machine Learning — Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html)\n",
    "- [SHAP repo](https://github.com/slundberg/shap) & [docs](https://shap.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "    !pip install eli5\n",
    "    !pip install pdpbox\n",
    "    !pip install shap\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_applied_modeling_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
